{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ccb03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomGrayscale(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees = 90),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "normalize_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65e93c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './catdog_data/test'\n",
    "train_path = './catdog_data/train'\n",
    "validation_path = './catdog_data/validation'\n",
    "\n",
    "train_data = ImageFolder(root = train_path, transform = train_transform)\n",
    "test_data = ImageFolder(root = test_path, transform = normalize_transform)\n",
    "validation_data = ImageFolder(root = validation_path, transform = normalize_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d48c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Memory allocated:\", round(torch.cuda.memory_allocated(0)/1024**3, 2), \"GB\")\n",
    "    print(\"Memory cached:\", round(torch.cuda.memory_reserved(0)/1024**3, 2), \"GB\")\n",
    "    print(\"Total memory:\", round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 2), \"GB\")\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs: {num_gpus}\")\n",
    "\n",
    "for i in range(num_gpus):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9849535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, activation_function):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 128, kernel_size = 3, padding = 1)\n",
    "        self.act1 = activation_function\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 128, out_channels = 64, kernel_size = 3, padding = 1)\n",
    "        self.act2 = activation_function\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size = 3, padding = 1)\n",
    "        self.act3 = activation_function\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 2, kernel_size = 3, padding = 1)\n",
    "        self.act4 = activation_function\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size = 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(2 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.act5 = activation_function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.act2(self.conv2(x))\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.act3(self.conv3(x))\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.act4(self.conv4(x))\n",
    "        x = self.maxpool4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.act5(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train(network, data_loader, criterion, optimizer, device):\n",
    "    network.train()\n",
    "    running_loss = 0.0\n",
    "    for data, target in data_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "    \n",
    "    return running_loss / len(data_loader.dataset)\n",
    "\n",
    "def validation(network, data_loader, criterion, device):\n",
    "    network.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = network(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    return val_loss / len(data_loader.dataset), 100 * correct / total\n",
    "\n",
    "results = []\n",
    "\n",
    "batch_sizes = [64, 128, 256]\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "\n",
    "activation_functions = {\n",
    "    'ReLU': nn.ReLU(),\n",
    "    'Sigmoid': nn.Sigmoid(),\n",
    "    'Tanh': nn.Tanh(),\n",
    "    'LeakyReLU': nn.LeakyReLU(),\n",
    "    'LogSigmoid': nn.LogSigmoid(),\n",
    "    'ELU': nn.ELU(),\n",
    "    'SiLU': nn.SiLU(),\n",
    "    'Softplus': nn.Softplus()\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    'Stochastic Gradient Descent': optim.SGD,\n",
    "    'Adam': optim.Adam\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 15\n",
    "\n",
    "for act_name, activation_function in activation_functions.items():\n",
    "    print(f\"Testing parameters for activation function {act_name}\")\n",
    "\n",
    "    activation_results = []\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\n=== Batch size: {batch_size} ===\")\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            dataset = train_data,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = True,\n",
    "            num_workers = 2\n",
    "        )\n",
    "\n",
    "        validation_dataloader = DataLoader(\n",
    "            dataset = validation_data,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = False,\n",
    "            num_workers = 2\n",
    "        )\n",
    "\n",
    "        test_dataloader = DataLoader(\n",
    "            dataset = test_data,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = False,\n",
    "            num_workers = 2\n",
    "        )\n",
    "\n",
    "        for learning_rate in learning_rates:\n",
    "            print(f\"\\n --- Learning rate: {learning_rate} ---\")\n",
    "\n",
    "            for opt_name, optimizer in optimizers.items():\n",
    "                print(f\"\\nOptimizing {act_name} with {opt_name}...\")\n",
    "\n",
    "                model = CNN(activation_function).to(device)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optimizer(model.parameters(), lr = learning_rate)\n",
    "\n",
    "                train_loss_history = []\n",
    "                val_loss_history = []\n",
    "                val_accuracy_history = []\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    train_loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "                    val_loss, val_accuracy = validation(model, validation_dataloader, criterion, device)\n",
    "\n",
    "                    train_loss_history.append(train_loss)\n",
    "                    val_loss_history.append(val_loss)\n",
    "                    val_accuracy_history.append(val_accuracy)\n",
    "\n",
    "                    print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f} %\")\n",
    "\n",
    "                results.append({\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"activation_function\": act_name,\n",
    "                    \"optimizer\": opt_name,\n",
    "                    \"train_loss_history\": train_loss_history,\n",
    "                    \"val_loss_history\": val_loss_history,\n",
    "                    \"val_accuracy_history\": val_accuracy_history\n",
    "                })\n",
    "\n",
    "                activation_results.append({\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"optimizer\": opt_name,\n",
    "                    \"train_loss_history\": train_loss_history,\n",
    "                    \"val_loss_history\": val_loss_history,\n",
    "                    \"val_accuracy_history\": val_accuracy_history\n",
    "                })\n",
    "\n",
    "    with open(f\"cnn_experiments_{act_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(activation_results, f)\n",
    "    \n",
    "    print(f\"\\nResults saved for {act_name}\")\n",
    "\n",
    "with open(f\"cnn_experiments_{act_name}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "print(\"\\nAll results saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a8fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"cnn_experiments.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "subset_relu = df[(df[\"activation_function\"] == \"ReLU\")]\n",
    "subset_sigmoid = df[(df[\"activation_function\"] == \"Sigmoid\")]\n",
    "subset_tanh = df[(df[\"activation_function\"] == \"Tanh\")]\n",
    "subset_leakyrelu = df[(df[\"activation_function\"] == \"LeakyReLU\")]\n",
    "subset_logsigmoid = df[(df[\"activation_function\"] == \"LogSigmoid\")]\n",
    "subset_elu = df[(df[\"activation_function\"] == \"ELU\")]\n",
    "subset_silu = df[(df[\"activation_function\"] == \"SiLU\")]\n",
    "subset_softplus = df[(df[\"activation_function\"] == \"Softplus\")]\n",
    "\n",
    "plt.figure()\n",
    "for _, row in subset_relu.iterrows():\n",
    "    label = f\"BS = {row['batch_size']}, LR = {row[learning_rate]}, OPT = {row['optimizer']}\"\n",
    "    plt.plot(data['val_accuracy_history'], label = label)\n",
    "plt.title(\"Validation Accuracy - ReLU\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for _, row in subset_sigmoid.iterrows():\n",
    "    label = f\"BS = {row['batch_size']}, LR = {row[learning_rate]}, OPT = {row['optimizer']}\"\n",
    "    plt.plot(data['val_accuracy_history'], label = label)\n",
    "plt.title(\"Validation Accuracy - Sigmoid\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for _, row in subset_tanh.iterrows():\n",
    "    label = f\"BS = {row['batch_size']}, LR = {row[learning_rate]}, OPT = {row['optimizer']}\"\n",
    "    plt.plot(data['val_accuracy_history'], label = label)\n",
    "plt.title(\"Validation Accuracy - Tanh\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for _, row in subset_leakyrelu.iterrows():\n",
    "    label = f\"BS = {row['batch_size']}, LR = {row[learning_rate]}, OPT = {row['optimizer']}\"\n",
    "    plt.plot(data['val_accuracy_history'], label = label)\n",
    "plt.title(\"Validation Accuracy - Leaky ReLU\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for _, row in subset_logsigmoid.iterrows():\n",
    "    label = f\"BS = {row['batch_size']}, LR = {row[learning_rate]}, OPT = {row['optimizer']}\"\n",
    "    plt.plot(data['val_accuracy_history'], label = label)\n",
    "plt.title(\"Validation Accuracy - LogSigmoid\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for _, row in subset_elu.iterrows():\n",
    "    label = f\"BS = {row['batch_size']}, LR = {row[learning_rate]}, OPT = {row['optimizer']}\"\n",
    "    plt.plot(data['val_accuracy_history'], label = label)\n",
    "plt.title(\"Validation Accuracy - ELU\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for _, row in subset_silu.iterrows():\n",
    "    label = f\"BS = {row['batch_size']}, LR = {row[learning_rate]}, OPT = {row['optimizer']}\"\n",
    "    plt.plot(data['val_accuracy_history'], label = label)\n",
    "plt.title(\"Validation Accuracy - SiLU\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for _, row in subset_softplus.iterrows():\n",
    "    label = f\"BS = {row['batch_size']}, LR = {row[learning_rate]}, OPT = {row['optimizer']}\"\n",
    "    plt.plot(data['val_accuracy_history'], label = label)\n",
    "plt.title(\"Validation Accuracy - Softplus\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds809",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
